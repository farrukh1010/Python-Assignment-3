{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhSY7Yt07UYN+xsfUJtQ46",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farrukh1010/Python-Assignment-3/blob/main/Copy_of_rag_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fD6qzjqBUha",
        "outputId": "c0a85c2a-d712-402d-c3be-72da44271771"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.5/2.5 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m158.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --quiet --upgrade langchain-text-splitters langchain-community langchain-google-genai  langchain-pinecone\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "userdata.get('GOOGLE_API_KEY')\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "XYpaxVUmB0nG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Retrieve the Google API key securely\n",
        "api_key = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Set the environment variable\n",
        "os.environ['GOOGLE_API_KEY'] = api_key\n",
        "\n",
        "# Print the API key\n",
        "if api_key:\n",
        "    print(\"Google API Key:\", api_key)\n",
        "else:\n",
        "    print(\"Google API Key not found!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCQHGNpqDCXh",
        "outputId": "5472e60f-e465-4e4a-d110-782d42bda25e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google API Key: AIzaSyCTMJ_00Z-oV8c2XMCiyGpybwR-otvsAhU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Ensure your VertexAI credentials are configured\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")"
      ],
      "metadata": {
        "id": "0CBsPDUGDL_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Initialize the model\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
        "\n",
        "# Test the model with a simple prompt\n",
        "try:\n",
        "    response = llm(\"Hello, how are you?\")\n",
        "    print(\"LLM Response:\", response)\n",
        "except Exception as e:\n",
        "    print(\"Error:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyBi6q-kDyLo",
        "outputId": "aa255c9e-2d12-46cf-b320-7c0a22740bec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Unexpected message with type <class 'str'> at the position 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-4b54becebe94>:8: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = llm(\"Hello, how are you?\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import HumanMessage\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Initialize the model\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
        "\n",
        "# Test the model with a structured prompt\n",
        "try:\n",
        "    # Create a human message\n",
        "    message = HumanMessage(content=\"Hello, how are you?\")\n",
        "\n",
        "    # Use the `invoke` method instead of calling the model directly\n",
        "    response = llm.invoke([message])\n",
        "\n",
        "    # Print the response\n",
        "    print(\"LLM Response:\", response.content)\n",
        "except Exception as e:\n",
        "    print(\"Error:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpkhcP-_EpAU",
        "outputId": "bbd85b7c-40a2-423e-bfa9-e4560cc7c6e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM Response: I am doing well, thank you for asking!  How are you today?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "# embeddings.embed_query(\"Hello world\")"
      ],
      "metadata": {
        "id": "TF7EOlnYE1xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector = embeddings.embed_query(\"we are buiding a RAG text\")"
      ],
      "metadata": {
        "id": "6-8719DsGFCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xlLiG-5lGIaS",
        "outputId": "a807beab-a679-4169-e6a6-fdd8b4e796fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.04122690111398697,\n",
              " -0.03249214589595795,\n",
              " -0.0529218465089798,\n",
              " -0.008323886431753635,\n",
              " -0.0003616840986069292]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from google.colab import userdata\n",
        "\n",
        "pinecone_api_key = userdata.get(\"PINECONE_API_KEY\")\n",
        "\n",
        "pc = Pinecone(api_key=pinecone_api_key)"
      ],
      "metadata": {
        "id": "8_L_lsyvFTeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pinecone_api_key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdoBeEzoFYAp",
        "outputId": "a5418dbb-4cf8-41d4-f41d-46083fe742df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pcsk_2uDWrR_2jmvP1BkhpPUPWADm1kyPmDjU41n7ds4NX382SRwqZjFZzvfxVad9xPSHZocWdi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "index_name = \"piaic-rag-project\"  # change if desired\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=768,\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
        "    )\n",
        "\n",
        "\n",
        "index = pc.Index(index_name)"
      ],
      "metadata": {
        "id": "zRBl08iMFs7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "vector_store = PineconeVectorStore(\n",
        "    index=index,\n",
        "    embedding=embeddings\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "T8XsDfJOJ_-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Save\n",
        "\n",
        "from uuid import uuid4\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "document_1 = Document(\n",
        "    page_content=\"I had chocalate chip pancakes and scrambled eggs for breakfast this morning.\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_2 = Document(\n",
        "    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "\n",
        "document_3 = Document(\n",
        "    page_content=\"Building an exciting new project with LangChain - come check it out!\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_4 = Document(\n",
        "    page_content=\"Robbers broke into the city bank and stole $1 million in cash.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "\n",
        "document_5 = Document(\n",
        "    page_content=\"Wow! That was an amazing movie. I can't wait to see it again.\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_6 = Document(\n",
        "    page_content=\"Is the new iPhone worth the price? Read this review to find out.\",\n",
        "    metadata={\"source\": \"website\"},\n",
        ")\n",
        "\n",
        "document_7 = Document(\n",
        "    page_content=\"The top 10 soccer players in the world right now.\",\n",
        "    metadata={\"source\": \"website\"},\n",
        ")\n",
        "\n",
        "document_8 = Document(\n",
        "    page_content=\"LangGraph is the best framework for building stateful, agentic applications!\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_9 = Document(\n",
        "    page_content=\"The stock market is down 500 points today due to fears of a recession.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "\n",
        "document_10 = Document(\n",
        "    page_content=\"I have a bad feeling I am going to get deleted :(\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "documents = [\n",
        "    document_1,\n",
        "    document_2,\n",
        "    document_3,\n",
        "    document_4,\n",
        "    document_5,\n",
        "    document_6,\n",
        "    document_7,\n",
        "    document_8,\n",
        "    document_9,\n",
        "    document_10,\n",
        "]"
      ],
      "metadata": {
        "id": "q8MToKssGZ45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cStaVsXeGceI",
        "outputId": "6ed19e35-3935-4691-bd7a-4d00dbc857e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "uuid4()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1jdVibbG2_N",
        "outputId": "fd6102d9-c3af-4461-fb51-182510de6b1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "UUID('90592de0-3981-4c2e-985f-0ec44f712f76')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
        "\n",
        "vector_store.add_documents(documents=documents, ids=uuids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wew4i3MKf_D",
        "outputId": "e4a4ac05-8f62-4636-9852-1f5def59c47e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ca4f9cdc-fc06-4bcc-b228-c014a3221f39',\n",
              " 'e33974bf-0e97-4f02-b2be-58839e1efbbe',\n",
              " 'f36b3c06-f6a5-4f07-8828-5f074d20e107',\n",
              " 'e438fef9-f8b3-40d9-80ac-29a6787bd5ff',\n",
              " '2e41bf03-4d12-47b5-be92-a993d9effe92',\n",
              " '78c8993e-9f98-4578-955b-0a2878b41373',\n",
              " 'c32970bc-2cb8-4c89-9351-38d8af8edaef',\n",
              " 'cf8e13ec-8bab-43a2-b8d4-62c4fca99c1f',\n",
              " 'ad5cba60-c305-46b1-a220-7e88e8d8d210',\n",
              " '76346fff-bdf9-4814-8e2e-e329a085a0ef']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Reterive\n",
        "results = vector_store.similarity_search(\n",
        "    \"LangChain provides abstractions to make working with LLMs easy\",\n",
        "   k=2,\n",
        "    filter={\"source\": \"tweet\"},\n",
        ")\n",
        "for res in results:\n",
        "    print(f\"* {res.page_content} [{res.metadata}]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1LPuWnIHFGX",
        "outputId": "f3e9c08a-219d-498a-8e10-7d5ab3fbb35b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* LangGraph is the best framework for building stateful, agentic applications! [{'source': 'tweet'}]\n",
            "* Building an exciting new project with LangChain - come check it out! [{'source': 'tweet'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from uuid import uuid4\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "# Step 1: Define documents\n",
        "documents = [\n",
        "    Document(page_content=\"I had chocolate chip pancakes and scrambled eggs for breakfast this morning.\", metadata={\"source\": \"tweet\"}),\n",
        "    Document(page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\", metadata={\"source\": \"news\"}),\n",
        "    Document(page_content=\"Building an exciting new project with LangChain - come check it out!\", metadata={\"source\": \"tweet\"}),\n",
        "    Document(page_content=\"Robbers broke into the city bank and stole $1 million in cash.\", metadata={\"source\": \"news\"}),\n",
        "    Document(page_content=\"Wow! That was an amazing movie. I can't wait to see it again.\", metadata={\"source\": \"tweet\"}),\n",
        "    Document(page_content=\"Is the new iPhone worth the price? Read this review to find out.\", metadata={\"source\": \"website\"}),\n",
        "    Document(page_content=\"The top 10 soccer players in the world right now.\", metadata={\"source\": \"website\"}),\n",
        "    Document(page_content=\"LangGraph is the best framework for building stateful, agentic applications!\", metadata={\"source\": \"tweet\"}),\n",
        "    Document(page_content=\"The stock market is down 500 points today due to fears of a recession.\", metadata={\"source\": \"news\"}),\n",
        "    Document(page_content=\"I have a bad feeling I am going to get deleted :(\", metadata={\"source\": \"tweet\"}),\n",
        "]\n",
        "\n",
        "# Step 2: Add documents to vector store\n",
        "uuids = [str(uuid4()) for _ in documents]\n",
        "vector_store.add_documents(documents=documents, ids=uuids)\n",
        "\n",
        "# Step 3: Define function to answer user queries\n",
        "def answer_to_user(query: str):\n",
        "    # Retrieve relevant document\n",
        "    results = vector_store.similarity_search(query, k=1)\n",
        "    if not results:\n",
        "        return \"No relevant documents found.\"\n",
        "\n",
        "    # Extract document content\n",
        "    document_content = results[0].page_content\n",
        "\n",
        "    # Define LLM and prompt\n",
        "    llm = GoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.7)\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"document\", \"query\"],\n",
        "        # template=\"Based on the following document, answer the user's query:\\n\\nDocument: {document}\\n\\nUser Query: {query}\\n\\nAnswer:\"\n",
        "         template=(\n",
        "            \"You are a helpful assistant. Use only the provided document to answer the user's query.\\n\\n\"\n",
        "            \"Document: {document}\\n\\n\"\n",
        "            \"User Query: {query}\\n\\n\"\n",
        "            \"Answer strictly based on the document content:\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "#     llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "\n",
        "#     # Generate and return the answer\n",
        "#     return llm_chain.run({\"document\": document_content, \"query\": query})\n",
        "\n",
        "# # Step 4: Example query\n",
        "# query = \"What is the weather forecast for tomorrow?\"\n",
        "# print(answer_to_user(query))\n",
        "    llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "\n",
        "    # Generate and return the answer\n",
        "    return llm_chain.run({\"document\": document_content, \"query\": query})\n",
        "\n",
        "# Step 4: Example query\n",
        "query = \"What is the weather forecast for tomorrow?\"\n",
        "print(answer_to_user(query))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njR5zOA7LOHW",
        "outputId": "5ce1a717-49cf-44d9-f3fc-6669375e6fad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}